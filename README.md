# CS5760 â€“ Natural Language Processing
## Homework 1 â€“ Regex, BPE, Bayes, Smoothing, and Tokenization

**Student Name:** Ajay Muppa  
**Student ID:** 700769264
**University:** University of Central Missouri  
**Course:** CS5760 â€“ Natural Language Processing  
**Semester:** Spring 2026  

---

## ðŸ“Œ Overview

This assignment covers core NLP concepts including:

- Regular Expressions (Regex) for pattern matching
- Byte Pair Encoding (BPE) for subword tokenization
- Bayes Rule for document classification
- Add-1 (Laplace) smoothing
- Tokenization challenges in Telugu (a morphologically rich language)

---

## ðŸ§© Q1 â€“ Regular Expressions (Regex)

| Task | Description | Regex |
|------|-------------|-------|
| ZIP Codes | Match US ZIP codes with optional +4 | `\b\d{5}(?:[- ]\d{4})?\b` |
| Lowercase Words | Words not starting with capital letters | `\b[a-z][A-Za-z]*(?:['-][A-Za-z]+)*\b` |
| Numbers | Signed, commas, decimals, scientific notation | `(?<!\w)[+-]?(?:\d{1,3}(?:,\d{3})+|\d+)(?:\.\d+)?(?:[eE][+-]?\d+)?(?!\w)` |
| Email Variants | email / e-mail / e mail | `(?i)\be(?:[ -â€“])?mail\b` |
| Interjection | go, goo, gooo with punctuation | `\bgo+\b[!.,?]?` |
| Line Ending | Lines ending with question mark and quotes | `^.*\?(?:[)\]"'â€™â€\]]*\s*)$` |

---

## ðŸ”¤ Q2 â€“ Byte Pair Encoding (BPE)

### Manual BPE (Toy Corpus)

- Added end-of-word marker `_`
- Calculated bigram frequencies
- Performed first three merges manually
- Updated vocabulary after each merge

### Mini BPE Implementation (Code)

- Learned merges programmatically
- Printed top pairs and vocabulary growth
- Segmented words:
  - `new`
  - `newer`
  - `lowest`
  - `widest`
  - `newestest`

### Key Insight

Subword tokenization solves the **Out-of-Vocabulary (OOV)** problem by breaking unseen words into known subword units. Learned tokens such as `er_` and `est_` align with meaningful morphemes.

---

## ðŸŒ Q2.3 â€“ BPE on Natural Paragraph

- Trained BPE on a custom paragraph
- Learned 30+ merges
- Observed subwords representing:
  - stems (`process`, `token`)
  - suffixes (`ing`, `ion`, `ies`)
  - complete frequent words

### Reflection

**Pros**
- Handles unseen words
- Captures morphological patterns

**Cons**
- Not always linguistically meaningful splits
- Longer token sequences

---

## ðŸ“˜ Q3 â€“ Bayes Rule for Documents

Bayes rule used for classification:

- **P(c)** â€“ Prior probability of class
- **P(d | c)** â€“ Likelihood of document given class
- **P(c | d)** â€“ Posterior probability used for classification

The denominator **P(d)** is ignored because it is constant across classes.

---

## âž• Q4 â€“ Add-1 Smoothing

Given:

- P(âˆ’) = 3/5, P(+) = 2/5
- Vocabulary size = 20
- Total negative tokens = 14

**Denominator with smoothing:**

14 + 20 = 34


**Probabilities:**

P(predictable | âˆ’) = (2 + 1) / 34 = 3 / 34
P(fun | âˆ’) = (0 + 1) / 34 = 1 / 34


---

## âœ‚ï¸ Q5 â€“ Tokenization in Telugu

### NaÃ¯ve vs Manual Tokenization

| NaÃ¯ve | Corrected |
|------|-----------|
| à°¬à°¾à°—à±à°‚à°¦à°¿! | à°¬à°¾à°—à±à°‚à°¦à°¿ + ! |
| à°’à°•à°Ÿà°¿. | à°’à°•à°Ÿà°¿ + . |
| à°šà±‚à°¡à°‚à°¡à°¿, | à°šà±‚à°¡à°‚à°¡à°¿ + , |
| à°‡à°·à±à°Ÿ à°ªà°¡à°¤à°¾à°°à±. | à°‡à°·à±à°Ÿ à°ªà°¡à°¤à°¾à°°à± + . |

### Tool Comparison

Used Indic NLP tokenizer. Output matched manual correction due to language-specific rules.

### Multiword Expressions (MWEs)

| Expression | Meaning |
|-----------|---------|
| à°¨à±à°¯à±‚ à°¢à°¿à°²à±à°²à±€ | Place name |
| à°ªà±à°°à°§à°¾à°¨ à°®à°‚à°¤à±à°°à°¿ | Title |
| à°šà±‡à°¤à±à°²à± à°Žà°¤à±à°¤à±‡à°¯à°¡à°‚ | Idiom (give up) |

MWEs should be treated as single tokens for better NLP performance.

### Reflection

Tokenization in Telugu is more complex than English due to:

- Rich morphology
- Suffix attachment
- Multiword expressions
- Ambiguous word boundaries

---

## ðŸ’» Repository Contents

- Regex implementations
- BPE implementation code
- Tokenization examples
- Explanations and reflections

---

## âœ… Submission

GitHub Repository:  
https://github.com/ajaymuppa/NLP_HW1

---
