{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOCmfhpzu5uLerw814rxzCP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ajaymuppa/NLP_HW1/blob/main/NLP_HW1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# question 2.2 task 1.\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "corpus = [\"new\", \"newer\", \"lowest\", \"widest\"]\n",
        "\n",
        "# initialize with characters + end-of-word\n",
        "def init_vocab(words):\n",
        "    return {tuple(list(w) + [\"_\"]): 1 for w in words}\n",
        "\n",
        "def get_pair_counts(vocab):\n",
        "    counts = Counter()\n",
        "    for tokens, freq in vocab.items():\n",
        "        for i in range(len(tokens)-1):\n",
        "            counts[(tokens[i], tokens[i+1])] += freq\n",
        "    return counts\n",
        "\n",
        "def merge_pair(pair, vocab):\n",
        "    new_vocab = {}\n",
        "    a, b = pair\n",
        "    for tokens, freq in vocab.items():\n",
        "        new = []\n",
        "        i = 0\n",
        "        while i < len(tokens):\n",
        "            if i < len(tokens)-1 and tokens[i] == a and tokens[i+1] == b:\n",
        "                new.append(a+b)\n",
        "                i += 2\n",
        "            else:\n",
        "                new.append(tokens[i])\n",
        "                i += 1\n",
        "        new_vocab[tuple(new)] = freq\n",
        "    return new_vocab\n",
        "\n",
        "vocab = init_vocab(corpus)\n",
        "symbols = set(ch for w in corpus for ch in w) | {\"_\"}\n",
        "\n",
        "for step in range(1, 8):\n",
        "    pair_counts = get_pair_counts(vocab)\n",
        "    if not pair_counts: break\n",
        "    top_pair = pair_counts.most_common(1)[0][0]\n",
        "    vocab = merge_pair(top_pair, vocab)\n",
        "    symbols.add(\"\".join(top_pair))\n",
        "    print(f\"Step {step}: top pair = {top_pair}, vocab size = {len(symbols)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OyBJw6RHU4WC",
        "outputId": "d7ffc1d0-d688-4047-ffdd-02ea0236a29c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1: top pair = ('n', 'e'), vocab size = 12\n",
            "Step 2: top pair = ('ne', 'w'), vocab size = 13\n",
            "Step 3: top pair = ('e', 's'), vocab size = 14\n",
            "Step 4: top pair = ('es', 't'), vocab size = 15\n",
            "Step 5: top pair = ('est', '_'), vocab size = 16\n",
            "Step 6: top pair = ('new', '_'), vocab size = 17\n",
            "Step 7: top pair = ('new', 'e'), vocab size = 18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "# -----------------------------\n",
        "# 2.3 — BPE on a short paragraph\n",
        "# -----------------------------\n",
        "\n",
        "PARAGRAPH = (\n",
        "    \"Natural language processing helps computers understand human language. \"\n",
        "    \"Language models learn patterns from large text collections. \"\n",
        "    \"Tokenization breaks words into smaller meaningful units. \"\n",
        "    \"Subword methods improve handling of unknown words. \"\n",
        "    \"Researchers continuously improve language technologies.\"\n",
        ")\n",
        "\n",
        "EOW = \"_\"          # end-of-word marker (assignment requirement)\n",
        "NUM_MERGES = 30    # learn at least 30 merges\n",
        "\n",
        "\n",
        "# ---------- Helpers ----------\n",
        "def words_from_paragraph(text: str):\n",
        "    # Basic word extraction: keep letters only, lowercase\n",
        "    cleaned = []\n",
        "    cur = []\n",
        "    for ch in text.lower():\n",
        "        if ch.isalpha():\n",
        "            cur.append(ch)\n",
        "        else:\n",
        "            if cur:\n",
        "                cleaned.append(\"\".join(cur))\n",
        "                cur = []\n",
        "    if cur:\n",
        "        cleaned.append(\"\".join(cur))\n",
        "    return cleaned\n",
        "\n",
        "def init_corpus(words):\n",
        "    # Represent each word as a list of characters + end-of-word marker\n",
        "    return [list(w) + [EOW] for w in words]\n",
        "\n",
        "def get_pair_counts(corpus):\n",
        "    counts = Counter()\n",
        "    for tokens in corpus:\n",
        "        for i in range(len(tokens) - 1):\n",
        "            counts[(tokens[i], tokens[i+1])] += 1\n",
        "    return counts\n",
        "\n",
        "def merge_pair_in_tokens(tokens, pair):\n",
        "    a, b = pair\n",
        "    merged = []\n",
        "    i = 0\n",
        "    while i < len(tokens):\n",
        "        if i < len(tokens) - 1 and tokens[i] == a and tokens[i+1] == b:\n",
        "            merged.append(a + b)\n",
        "            i += 2\n",
        "        else:\n",
        "            merged.append(tokens[i])\n",
        "            i += 1\n",
        "    return merged\n",
        "\n",
        "def apply_merge_to_corpus(corpus, pair):\n",
        "    return [merge_pair_in_tokens(tokens, pair) for tokens in corpus]\n",
        "\n",
        "def learn_bpe(words, num_merges=30):\n",
        "    corpus = init_corpus(words)\n",
        "    merges = []              # list of pairs in the order learned\n",
        "    merge_freqs = []         # frequency of the chosen pair at that step\n",
        "\n",
        "    # initial vocabulary = all unique symbols (characters + EOW)\n",
        "    vocab = set(sym for w in corpus for sym in w)\n",
        "\n",
        "    for step in range(1, num_merges + 1):\n",
        "        pair_counts = get_pair_counts(corpus)\n",
        "        if not pair_counts:\n",
        "            break\n",
        "\n",
        "        top_pair, top_count = pair_counts.most_common(1)[0]\n",
        "\n",
        "        # record\n",
        "        merges.append(top_pair)\n",
        "        merge_freqs.append((top_pair, top_count))\n",
        "\n",
        "        # merge everywhere in corpus\n",
        "        corpus = apply_merge_to_corpus(corpus, top_pair)\n",
        "\n",
        "        # add merged symbol to vocabulary\n",
        "        vocab.add(top_pair[0] + top_pair[1])\n",
        "\n",
        "        print(f\"Step {step:02d}: top pair = {top_pair}  |  count = {top_count:3d}  |  vocab size = {len(vocab)}\")\n",
        "\n",
        "    return merges, merge_freqs, vocab\n",
        "\n",
        "def bpe_segment_word(word, merges):\n",
        "    # segmenter: apply merges greedily, in learned order\n",
        "    tokens = list(word.lower()) + [EOW]\n",
        "    for pair in merges:\n",
        "        tokens = merge_pair_in_tokens(tokens, pair)\n",
        "    return tokens\n",
        "\n",
        "\n",
        "# ---------- Run BPE ----------\n",
        "words = words_from_paragraph(PARAGRAPH)\n",
        "merges, merge_freqs, vocab = learn_bpe(words, NUM_MERGES)\n",
        "\n",
        "print(\"\\n--- Five most frequent merges (by frequency at time learned) ---\")\n",
        "top5_merges = sorted(merge_freqs, key=lambda x: x[1], reverse=True)[:5]\n",
        "for pair, cnt in top5_merges:\n",
        "    print(f\"{pair}  ->  {pair[0] + pair[1]}   (count={cnt})\")\n",
        "\n",
        "print(\"\\n--- Five longest subword tokens in final vocabulary ---\")\n",
        "# longest tokens by string length (excluding single chars if you want, but we keep all)\n",
        "longest5 = sorted(vocab, key=lambda s: (len(s), s), reverse=True)[:5]\n",
        "for tok in longest5:\n",
        "    print(tok)\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMMcuwToVDQN",
        "outputId": "c5e2cba1-ce11-4f8e-d996-2e8ac7272bbe"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 01: top pair = ('s', '_')  |  count =  12  |  vocab size = 26\n",
            "Step 02: top pair = ('a', 'n')  |  count =   8  |  vocab size = 27\n",
            "Step 03: top pair = ('e', '_')  |  count =   7  |  vocab size = 28\n",
            "Step 04: top pair = ('g', 'e_')  |  count =   5  |  vocab size = 29\n",
            "Step 05: top pair = ('i', 'n')  |  count =   5  |  vocab size = 30\n",
            "Step 06: top pair = ('e', 'r')  |  count =   5  |  vocab size = 31\n",
            "Step 07: top pair = ('l', 'an')  |  count =   4  |  vocab size = 32\n",
            "Step 08: top pair = ('lan', 'g')  |  count =   4  |  vocab size = 33\n",
            "Step 09: top pair = ('lang', 'u')  |  count =   4  |  vocab size = 34\n",
            "Step 10: top pair = ('langu', 'a')  |  count =   4  |  vocab size = 35\n",
            "Step 11: top pair = ('langua', 'ge_')  |  count =   4  |  vocab size = 36\n",
            "Step 12: top pair = ('r', 'o')  |  count =   4  |  vocab size = 37\n",
            "Step 13: top pair = ('a', 't')  |  count =   3  |  vocab size = 38\n",
            "Step 14: top pair = ('p', 'ro')  |  count =   3  |  vocab size = 39\n",
            "Step 15: top pair = ('in', 'g')  |  count =   3  |  vocab size = 40\n",
            "Step 16: top pair = ('c', 'o')  |  count =   3  |  vocab size = 41\n",
            "Step 17: top pair = ('u', 'n')  |  count =   3  |  vocab size = 42\n",
            "Step 18: top pair = ('e', 'a')  |  count =   3  |  vocab size = 43\n",
            "Step 19: top pair = ('n', '_')  |  count =   3  |  vocab size = 44\n",
            "Step 20: top pair = ('w', 'o')  |  count =   3  |  vocab size = 45\n",
            "Step 21: top pair = ('wo', 'r')  |  count =   3  |  vocab size = 46\n",
            "Step 22: top pair = ('wor', 'd')  |  count =   3  |  vocab size = 47\n",
            "Step 23: top pair = ('a', 'l')  |  count =   2  |  vocab size = 48\n",
            "Step 24: top pair = ('e', 's')  |  count =   2  |  vocab size = 49\n",
            "Step 25: top pair = ('ing', '_')  |  count =   2  |  vocab size = 50\n",
            "Step 26: top pair = ('e', 'l')  |  count =   2  |  vocab size = 51\n",
            "Step 27: top pair = ('t', 'er')  |  count =   2  |  vocab size = 52\n",
            "Step 28: top pair = ('an', 'd')  |  count =   2  |  vocab size = 53\n",
            "Step 29: top pair = ('o', 'd')  |  count =   2  |  vocab size = 54\n",
            "Step 30: top pair = ('ea', 'r')  |  count =   2  |  vocab size = 55\n",
            "\n",
            "--- Five most frequent merges (by frequency at time learned) ---\n",
            "('s', '_')  ->  s_   (count=12)\n",
            "('a', 'n')  ->  an   (count=8)\n",
            "('e', '_')  ->  e_   (count=7)\n",
            "('g', 'e_')  ->  ge_   (count=5)\n",
            "('i', 'n')  ->  in   (count=5)\n",
            "\n",
            "--- Five longest subword tokens in final vocabulary ---\n",
            "language_\n",
            "langua\n",
            "langu\n",
            "word\n",
            "lang\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- Segment 5 words from the paragraph ----------\n",
        "# Pick 5 words present in the paragraph; include one rare and one derived/inflected form.\n",
        "# (Example choices: \"tokenization\" (derived), \"collections\" (rarer), plus a few others)\n",
        "targets = [\"tokenization\", \"collections\", \"processing\", \"unknown\", \"technologies\"]\n",
        "\n",
        "print(\"\\n--- Segmentations (tokens include end-of-word _ where applicable) ---\")\n",
        "for w in targets:\n",
        "    seg = bpe_segment_word(w, merges)\n",
        "    print(f\"{w:14s} -> {' '.join(seg)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "utPPu6CAVTRV",
        "outputId": "a24c9033-c4e8-4b1c-a25e-fb947714518d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Segmentations (tokens include end-of-word _ where applicable) ---\n",
            "tokenization   -> t o k e n i z at i o n_\n",
            "collections    -> co l l e c t i o n s_\n",
            "processing     -> pro c es s ing_\n",
            "unknown        -> un k n o w n_\n",
            "technologies   -> t e c h n o l o g i e s_\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 5. 1.\n",
        "# Naive space-based tokenization (Telugu or any language)\n",
        "# Splits ONLY on whitespace, so punctuation stays attached.\n",
        "\n",
        "text = \"నేను ఈ సినిమా చాలా బాగుంది! ఇది ఈ సంవత్సరం చూసిన ఉత్తమ చిత్రాల్లో ఒకటి. చూడండి, మీరు ఖచ్చితంగా ఇష్టపడతారు.\"\n",
        "\n",
        "tokens = text.split()   # naive whitespace split\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vrd78Bh7VXGF",
        "outputId": "5fecae14-7fbf-4b67-d7f3-1b05aa9abf15"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['నేను', 'ఈ', 'సినిమా', 'చాలా', 'బాగుంది!', 'ఇది', 'ఈ', 'సంవత్సరం', 'చూసిన', 'ఉత్తమ', 'చిత్రాల్లో', 'ఒకటి.', 'చూడండి,', 'మీరు', 'ఖచ్చితంగా', 'ఇష్టపడతారు.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#question 5.2. Tool comparision\n",
        "\n",
        "from indicnlp.tokenize import indic_tokenize\n",
        "\n",
        "text = \"నేను ఈ సినిమా చాలా బాగుంది! ఇది ఈ సంవత్సరం చూసిన ఉత్తమ చిత్రాల్లో ఒకటి. చూడండి, మీరు ఖచ్చితంగా ఇష్టపడతారు.\"\n",
        "\n",
        "tool_tokens = indic_tokenize.trivial_tokenize(text, lang=\"te\")\n",
        "print(tool_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7oumApNVa_M",
        "outputId": "723cfe7a-9469-4524-8c63-dffaf4f12896"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['నేను', 'ఈ', 'సినిమా', 'చాలా', 'బాగుంది', '!', 'ఇది', 'ఈ', 'సంవత్సరం', 'చూసిన', 'ఉత్తమ', 'చిత్రాల్లో', 'ఒకటి', '.', 'చూడండి', ',', 'మీరు', 'ఖచ్చితంగా', 'ఇష్టపడతారు', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q2izzzyeVh1U"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}